# Flutter Google STT

**cÃ¡c bÆ°á»›c cáº§n lÃ m Ä‘á»ƒ triá»ƒn khai MVP STT (speech-to-text) trong app Flutter**:

---

## ğŸ“ CÃ¡c bÆ°á»›c thá»±c hiá»‡n

### 1. Chuáº©n bá»‹ mÃ´i trÆ°á»ng

* Quyáº¿t Ä‘á»‹nh backend STT ban Ä‘áº§u: **Google Speech-to-Text API** (dÃ¹ng free credit)
* Táº¡o **Google Cloud Project**, báº­t **Speech-to-Text API**, láº¥y **API Key / Service Account JSON**.
* ThÃªm package Flutter Ä‘á»ƒ gá»i API (vÃ­ dá»¥ `googleapis`, `http`, hoáº·c gRPC client).

---

### 2. Xá»­ lÃ½ audio táº¡i client (Flutter app)

* Thu Ã¢m báº±ng `flutter_sound` hoáº·c `record` plugin.
* Chia audio thÃ nh **chunk nhá»** (5â€“10 giÃ¢y) Ä‘á»ƒ giáº£m latency.
* Tiá»n xá»­ lÃ½:

  * **Noise reduction** (lá»c nhiá»…u cÆ¡ báº£n).
  * **Silence detection** (loáº¡i bá» quÃ£ng láº·ng khÃ´ng cáº§n thiáº¿t).
  * **Format chuáº©n**: PCM LINEAR16 hoáº·c FLAC, 16kHzâ€“48kHz mono.

---

**rnnoise** source rá»“i. ÄÃ¢y lÃ  lib C (DSP + RNN) Ä‘á»ƒ **noise suppression** cháº¡y **local on-device**.

Äá»ƒ dÃ¹ng Ä‘Æ°á»£c trong Flutter thÃ¬ báº¡n sáº½ **khÃ´ng gá»i trá»±c tiáº¿p máº¥y file `.c` Ä‘Ã¢u**, mÃ  cáº§n **build ra thÆ° viá»‡n Ä‘á»™ng (.so / .a / .dll)** rá»“i wrap vÃ o Flutter qua **FFI hoáº·c Platform Channel**.

---

### 1. Nhá»¯ng file nÃ o quan trá»ng Ä‘á»ƒ build rnnoise

Trong folder báº¡n list:

* **Core DSP + RNN**

  * `denoise.c / denoise.h` â†’ main noise suppression API (`rnnoise_process_frame`)
  * `rnn.c / rnn.h` â†’ model runner
  * `nnet.c / nnet.h / nnet_default.c` â†’ neural net weights
  * `rnnoise_tables.c` â†’ pre-trained model weights
* **Helper / dependencies**

  * `kiss_fft.c / kiss_fft.h / _kiss_fft_guts.h` â†’ FFT implementation
  * `celt_lpc.c / celt_lpc.h` â†’ linear predictive coding
  * `vec_*.h` â†’ SIMD optimizations (AVX, NEON)
* **Not needed for runtime**

  * `dump_features.c`, `dump_rnnoise_tables.c`, `rnn_train.py`, `write_weights.c`, `parse_lpcnet_weights.c` â†’ training / debug tools

ğŸ‘‰ Vá»›i Flutter app (cháº¡y model Ä‘Ã£ pre-trained), báº¡n chá»‰ cáº§n:

* `denoise.c`
* `rnn.c`
* `nnet.c` (vá»›i `nnet_default.c` chá»©a weights máº·c Ä‘á»‹nh)
* `kiss_fft.c`
* `celt_lpc.c`
* cÃ¹ng cÃ¡c `.h` liÃªn quan

---

### 2. Build thÃ nh native library

#### ğŸ”¹ Android (NDK)

* Viáº¿t `Android.mk` hoáº·c `CMakeLists.txt` â†’ compile thÃ nh `librnnoise.so`.
* DÃ¹ng `ndk-build` hoáº·c Gradle vá»›i CMake.

VÃ­ dá»¥ `CMakeLists.txt`:

```cmake
cmake_minimum_required(VERSION 3.4.1)

add_library(rnnoise SHARED
    denoise.c
    rnn.c
    nnet.c
    kiss_fft.c
    celt_lpc.c
    rnnoise_tables.c)

target_include_directories(rnnoise PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})
```

#### ğŸ”¹ iOS

* Táº¡o Xcode static lib project, build thÃ nh `.a` (libRNNoise.a).
* Link vÃ o Flutter iOS Runner project.

---

### 3. Expose API ra Flutter

Trong `denoise.h` cÃ³ hÃ m chÃ­nh:

```c
typedef struct RNNState RNNState;

RNNState *rnnoise_create(void);
void rnnoise_destroy(RNNState *st);
float rnnoise_process_frame(RNNState *st, float *out, const float *in);
```

* `rnnoise_process_frame` xá»­ lÃ½ 1 **frame 480 máº«u** (30ms @ 16kHz).
* Input/Output lÃ  float array (mono).

Flutter gá»i qua **Dart FFI**:

```dart
final rnnoiseLib = DynamicLibrary.open("librnnoise.so");

typedef rnnoise_create_c = Pointer<Void> Function();
typedef rnnoise_create_dart = Pointer<Void> Function();
final rnnoiseCreate = rnnoiseLib
    .lookupFunction<rnnoise_create_c, rnnoise_create_dart>("rnnoise_create");
```

---

### 4. Workflow trong Flutter

1. Ghi Ã¢m mic â†’ PCM16 â†’ convert sang float 16kHz mono.
2. Chunk thÃ nh **480 samples** â†’ gá»i `rnnoise_process_frame`.
3. Nháº­n frame sáº¡ch â†’ append láº¡i thÃ nh audio stream.
4. Stream sáº¡ch nÃ y má»›i gá»­i Ä‘i **Google STT / Whisper**.


---

### 3. Gá»­i dá»¯ liá»‡u Ä‘áº¿n API

CÃ³ 2 cháº¿ Ä‘á»™ chÃ­nh:

* **Streaming** (real-time): app gá»­i audio chunk liÃªn tá»¥c Ä‘áº¿n Google STT â†’ tráº£ transcript gáº§n nhÆ° live.
* **Batch (async)**: app thu xong file dÃ i â†’ gá»­i cáº£ file â†’ nháº­n transcript hoÃ n chá»‰nh.

ğŸ‘‰ MVP cá»§a báº¡n hÆ°á»›ng Ä‘áº¿n **cuá»™c há»p dÃ i**, váº­y há»£p lÃ½ nháº¥t lÃ :

* Gá»­i **chunk theo thá»i gian thá»±c** (Streaming API).
* LÆ°u transcript raw vÃ o DB/local Ä‘á»ƒ háº­u xá»­ lÃ½ sau.

---

### 4. Háº­u xá»­ lÃ½ transcript

* Gom cÃ¡c chunk transcript â†’ thÃ nh vÄƒn báº£n thÃ´.
* LÃ m **cleanup**: sá»­a chÃ­nh táº£, loáº¡i bá» tá»« thá»«a, lá»c nhiá»…u.
* CÃ³ thá»ƒ cháº¡y **speaker diarization** (nháº­n diá»‡n ngÆ°á»i nÃ³i) náº¿u cáº§n phÃ¢n biá»‡t ai Ä‘ang nÃ³i.

---

### 5. TÃ³m táº¯t & táº¡o task

* Sau khi cÃ³ transcript â€œsáº¡châ€:

  * Gá»­i qua **LLM / NLP service** (vÃ­ dá»¥ OpenAI GPT, Gemini, hoáº·c model tá»± train) Ä‘á»ƒ tÃ³m táº¯t ná»™i dung.
  * Sinh **action items / tasks** tá»« ná»™i dung.

---

### 6. TÃ­ch há»£p UI Flutter

* UI hiá»ƒn thá»‹ transcript **theo thá»i gian thá»±c** (subtitle style).
* CÃ³ cháº¿ Ä‘á»™ xem **toÃ n bá»™ transcript sau cuá»™c há»p**.
* Cho phÃ©p **xuáº¥t báº£n tÃ³m táº¯t + tasks** (PDF, email, share).

---

### 7. Tá»‘i Æ°u hÃ³a chi phÃ­

* Google STT free credit â†’ test MVP.
* Sau Ä‘Ã³ so sÃ¡nh chi phÃ­ Google STT vs Whisper API (OpenAI hoáº·c self-host).
* CÃ¢n nháº¯c **hybrid**:

  * Google STT cho real-time.
  * Whisper cho batch cleanup (offline ráº» hÆ¡n).

---

ğŸ‘‰ Tá»•ng quÃ¡t flow:

ğŸ¤ Thu Ã¢m â†’ ğŸ›ï¸ Xá»­ lÃ½ noise/silence â†’ ğŸ“¤ Gá»­i chunk (Streaming API) â†’ ğŸ“„ Transcript raw â†’ ğŸ§¹ Cleanup â†’ ğŸ“‘ TÃ³m táº¯t + Tasks â†’ ğŸ“± Hiá»ƒn thá»‹ trong app

---

## Voice Activity Detection (VAD) Implementation

This project includes two Voice Activity Detection (VAD) implementations:

### 1. Simulated VAD

The initial implementation uses a basic amplitude-based approach to detect voice activity:

* Uses WebRTC audio processing
* Detects speech based on audio amplitude thresholds
* Available in `VadService` and `VadTestingScreen`

### 2. Package-based VAD (Silero VAD)

A more advanced implementation using the `vad` Flutter package that provides ML-based VAD:

* Uses Silero VAD models (v4 and v5)
* More accurate speech detection
* Cross-platform support (iOS, Android, Web)
* Available in `PackageVadService` and `PackageVADScreen`

### How to Use the Package-based VAD

Our `PackageVadService` implementation provides:

* **Real-time audio visualization**: Waveforms and confidence levels
* **Automatic speech segmentation**: Captures individual utterances
* **Audio recording management**: Records and saves speech segments
* **Event logging**: Tracks all speech detection events

The implementation exposes several streams for UI updates:

```dart
// Audio level updates for visualization
vadService.audioLevelStream.listen((level) {
  // Update waveform UI
});

// Speech detection state changes
vadService.speechDetectedStream.listen((isSpeech) {
  // Update UI to show active speech state
});

// Access to waveform data
vadService.waveformStream.listen((waveformData) {
  // Update detailed waveform visualization
});
```

The VAD handler provides these core event streams:

```dart
// When speech is first detected
vadHandler.onSpeechStart.listen((_) {
  print('Speech detected');
});

// When speech is confirmed (not a misfire)
vadHandler.onRealSpeechStart.listen((_) {
  print('Real speech confirmed');
});

// When speech ends
vadHandler.onSpeechEnd.listen((samples) {
  print('Speech ended, processing audio segment');
});

// For processing each audio frame
vadHandler.onFrameProcessed.listen((frameData) {
  // Update UI with confidence score: frameData.isSpeech
  // Process audio data: frameData.frame
});
```

### Web Implementation Notes

For web support, the following scripts are added to `web/index.html`:

```html
<!-- In <head> -->
<script src="assets/packages/vad/assets/ort.js"></script>
<script src="enable-threads.js"></script>

<!-- Before </body> -->
<script src="assets/packages/vad/assets/bundle.min.js" defer></script>
<script src="assets/packages/vad/assets/vad_web.js" defer></script>
```

The `enable-threads.js` script helps enable WebAssembly threading for local development.

### VAD Configuration Options

```dart
await vadHandler.startListening(
  model: 'v5',                    // 'legacy' or 'v5'
  frameSamples: 512,              // 512 for v5, 1536 for legacy
  positiveSpeechThreshold: 0.5,   // Speech detection threshold
  negativeSpeechThreshold: 0.3,   // Silence detection threshold
  minSpeechFrames: 2,             // Minimum frames for valid speech
);
```

### The PackageVADScreen UI

The `PackageVADScreen` component provides a comprehensive UI for testing and visualizing VAD:

* **Status Indicator**: Shows active speech detection state
* **Audio Waveform**: Real-time visualization of audio levels
* **Event Logs**: Chronological list of all VAD events with timestamps
* **Audio Segments**: List of captured speech segments with playback options
* **Control Panel**: Start/stop recording and adjust VAD settings

To include this screen in your application:

```dart
Navigator.push(
  context,
  MaterialPageRoute(builder: (context) => const PackageVADScreen()),
);
```

### Integration with STT Pipeline

To integrate the VAD service with a Speech-to-Text pipeline:

1. Start the VAD service and recording
2. Listen to the `onSpeechEnd` event to get completed audio segments
3. Send these segments to your STT service
4. Process the transcription results

Example:

```dart
// Initialize VAD service
final vadService = PackageVadService();

// Setup STT processing
vadHandler.onSpeechEnd.listen((samples) {
  // Get the latest completed audio segment
  final latestSegment = vadService.segments.last;
  
  // Send to STT service
  sendAudioToSTT(latestSegment.path);
});
```
